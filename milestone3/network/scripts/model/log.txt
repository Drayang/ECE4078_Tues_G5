Net Architecture:
Resnet18Skip(
  (res18_backbone): Sequential(
    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  )
  (conv2_x): Sequential(
    (0): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv3_x): Sequential(
    (0): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv4_x): Sequential(
    (0): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv5_x): Sequential(
    (0): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (top_conv): Sequential(
    (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU()
  )
  (lateral_conv1): Sequential(
    (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU()
  )
  (lateral_conv2): Sequential(
    (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU()
  )
  (lateral_conv3): Sequential(
    (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU()
  )
  (segmentation_conv): Sequential(
    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (2): Conv2d(64, 6, kernel_size=(1, 1), stride=(1, 1))
  )
  (criterion): CrossEntropyLoss()
)
Loss Function: CrossEntropyLoss

===========================================================
==================== Hyper-parameters =====================
n_classes: 5
lr: 0.001
epochs: 40
batch_size: 64
weight_decay: 0.0001
scheduler_step: 5
scheduler_gamma: 0.5
model_dir: model
load_best: 0
log_freq: 20
dataset_dir: dataset
===========================================================
============= Epoch 0 | 2022-09-02 16:41:03 ===============
=> Current Lr: 0.001
[0/99]: 1.7258
[20/99]: 0.5339
[40/99]: 0.4107
[60/99]: 0.3828Net Architecture:
Resnet18Skip(
  (res18_backbone): Sequential(
    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  )
  (conv2_x): Sequential(
    (0): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv3_x): Sequential(
    (0): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv4_x): Sequential(
    (0): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv5_x): Sequential(
    (0): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (top_conv): Sequential(
    (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU()
  )
  (lateral_conv1): Sequential(
    (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU()
  )
  (lateral_conv2): Sequential(
    (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU()
  )
  (lateral_conv3): Sequential(
    (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU()
  )
  (segmentation_conv): Sequential(
    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (2): Conv2d(64, 6, kernel_size=(1, 1), stride=(1, 1))
  )
  (criterion): CrossEntropyLoss()
)
Loss Function: CrossEntropyLoss

===========================================================
==================== Hyper-parameters =====================
n_classes: 5
lr: 0.001
epochs: 40
batch_size: 64
weight_decay: 0.0001
scheduler_step: 5
scheduler_gamma: 0.5
model_dir: model
load_best: 0
log_freq: 20
dataset_dir: dataset
===========================================================
============= Epoch 0 | 2022-09-02 17:05:09 ===============
=> Current Lr: 0.001Net Architecture:
Resnet18Skip(
  (res18_backbone): Sequential(
    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  )
  (conv2_x): Sequential(
    (0): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv3_x): Sequential(
    (0): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv4_x): Sequential(
    (0): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv5_x): Sequential(
    (0): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (top_conv): Sequential(
    (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU()
  )
  (lateral_conv1): Sequential(
    (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU()
  )
  (lateral_conv2): Sequential(
    (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU()
  )
  (lateral_conv3): Sequential(
    (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU()
  )
  (segmentation_conv): Sequential(
    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (2): Conv2d(64, 6, kernel_size=(1, 1), stride=(1, 1))
  )
  (criterion): CrossEntropyLoss()
)
Loss Function: CrossEntropyLoss

===========================================================
==================== Hyper-parameters =====================
n_classes: 5
lr: 0.001
epochs: 40
batch_size: 64
weight_decay: 0.0001
scheduler_step: 5
scheduler_gamma: 0.5
model_dir: model
load_best: 0
log_freq: 20
dataset_dir: dataset
===========================================================
============= Epoch 0 | 2022-09-02 17:08:05 ===============
=> Current Lr: 0.001Net Architecture:
Resnet18Skip(
  (res18_backbone): Sequential(
    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  )
  (conv2_x): Sequential(
    (0): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv3_x): Sequential(
    (0): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv4_x): Sequential(
    (0): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv5_x): Sequential(
    (0): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (top_conv): Sequential(
    (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU()
  )
  (lateral_conv1): Sequential(
    (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU()
  )
  (lateral_conv2): Sequential(
    (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU()
  )
  (lateral_conv3): Sequential(
    (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU()
  )
  (segmentation_conv): Sequential(
    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (2): Conv2d(64, 6, kernel_size=(1, 1), stride=(1, 1))
  )
  (criterion): CrossEntropyLoss()
)
Loss Function: CrossEntropyLoss

===========================================================
==================== Hyper-parameters =====================
n_classes: 5
lr: 0.001
epochs: 40
batch_size: 64
weight_decay: 0.0001
scheduler_step: 5
scheduler_gamma: 0.5
model_dir: model
load_best: 0
log_freq: 20
dataset_dir: dataset
===========================================================
============= Epoch 0 | 2022-09-02 17:09:48 ===============
=> Current Lr: 0.001Net Architecture:
Resnet18Skip(
  (res18_backbone): Sequential(
    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  )
  (conv2_x): Sequential(
    (0): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv3_x): Sequential(
    (0): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv4_x): Sequential(
    (0): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv5_x): Sequential(
    (0): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (top_conv): Sequential(
    (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU()
  )
  (lateral_conv1): Sequential(
    (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU()
  )
  (lateral_conv2): Sequential(
    (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU()
  )
  (lateral_conv3): Sequential(
    (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU()
  )
  (segmentation_conv): Sequential(
    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (2): Conv2d(64, 6, kernel_size=(1, 1), stride=(1, 1))
  )
  (criterion): CrossEntropyLoss()
)
Loss Function: CrossEntropyLoss

===========================================================
==================== Hyper-parameters =====================
n_classes: 5
lr: 0.001
epochs: 40
batch_size: 64
weight_decay: 0.0001
scheduler_step: 5
scheduler_gamma: 0.5
model_dir: model
load_best: 0
log_freq: 20
dataset_dir: dataset
===========================================================
============= Epoch 0 | 2022-09-02 17:11:48 ===============
=> Current Lr: 0.001Net Architecture:
Resnet18Skip(
  (res18_backbone): Sequential(
    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  )
  (conv2_x): Sequential(
    (0): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv3_x): Sequential(
    (0): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv4_x): Sequential(
    (0): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv5_x): Sequential(
    (0): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (top_conv): Sequential(
    (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU()
  )
  (lateral_conv1): Sequential(
    (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU()
  )
  (lateral_conv2): Sequential(
    (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU()
  )
  (lateral_conv3): Sequential(
    (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU()
  )
  (segmentation_conv): Sequential(
    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (2): Conv2d(64, 6, kernel_size=(1, 1), stride=(1, 1))
  )
  (criterion): CrossEntropyLoss()
)
Loss Function: CrossEntropyLoss

===========================================================
==================== Hyper-parameters =====================
n_classes: 5
lr: 0.001
epochs: 40
batch_size: 32
weight_decay: 0.0001
scheduler_step: 5
scheduler_gamma: 0.5
model_dir: model
load_best: 0
log_freq: 20
dataset_dir: dataset
===========================================================
============= Epoch 0 | 2022-09-02 17:14:51 ===============
=> Current Lr: 0.001
[0/198]: 1.7823
[20/198]: 0.4969
[40/198]: 0.3907
[60/198]: 0.3648
[80/198]: 0.3308
[100/198]: 0.2856
[120/198]: 0.3505
[140/198]: 0.3313
[160/198]: 0.2342
[180/198]: 0.2854
=> Training Loss: 0.3640, Evaluation Loss 0.3567

============= Epoch 1 | 2022-09-02 17:16:08 ===============
=> Current Lr: 0.001
[0/198]: 0.3424
[20/198]: 0.3452
[40/198]: 0.1853
[60/198]: 0.1855
[80/198]: 0.2176
[100/198]: 0.2198
[120/198]: 0.2286
[140/198]: 0.1916
[160/198]: 0.1554
[180/198]: 0.2107
=> Training Loss: 0.2204, Evaluation Loss 0.1851

============= Epoch 2 | 2022-09-02 17:17:13 ===============
=> Current Lr: 0.001
[0/198]: 0.1795
[20/198]: 0.2041
[40/198]: 0.1273
[60/198]: 0.1516
[80/198]: 0.1407
[100/198]: 0.1687
[120/198]: 0.1342
[140/198]: 0.1053
[160/198]: 0.2110
[180/198]: 0.1795
=> Training Loss: 0.1617, Evaluation Loss 0.1246

============= Epoch 3 | 2022-09-02 17:18:18 ===============
=> Current Lr: 0.001
[0/198]: 0.1095
[20/198]: 0.1429
[40/198]: 0.0899
[60/198]: 0.2014
[80/198]: 0.1095
[100/198]: 0.1341
[120/198]: 0.2523
[140/198]: 0.2816
[160/198]: 0.1707
[180/198]: 0.0914
=> Training Loss: 0.1297, Evaluation Loss 0.0939

============= Epoch 4 | 2022-09-02 17:19:23 ===============
=> Current Lr: 0.001
[0/198]: 0.0511
[20/198]: 0.2043
[40/198]: 0.0850
[60/198]: 0.1207
[80/198]: 0.0553
[100/198]: 0.1111
[120/198]: 0.0679
[140/198]: 0.0630
[160/198]: 0.1440
[180/198]: 0.1031
=> Training Loss: 0.0980, Evaluation Loss 0.1003

============= Epoch 5 | 2022-09-02 17:20:32 ===============
=> Current Lr: 0.0005
[0/198]: 0.1365
[20/198]: 0.0537
[40/198]: 0.1546
[60/198]: 0.0454
[80/198]: 0.0755
[100/198]: 0.0671
[120/198]: 0.0538
[140/198]: 0.0988
[160/198]: 0.1335
[180/198]: 0.0545
=> Training Loss: 0.0648, Evaluation Loss 0.0561

============= Epoch 6 | 2022-09-02 17:21:41 ===============
=> Current Lr: 0.0005
[0/198]: 0.0694
[20/198]: 0.0672
[40/198]: 0.0347
[60/198]: 0.0609
[80/198]: 0.0326
[100/198]: 0.0881
[120/198]: 0.0881
[140/198]: 0.0454
[160/198]: 0.0493
[180/198]: 0.0690
=> Training Loss: 0.0567, Evaluation Loss 0.0706

============= Epoch 7 | 2022-09-02 17:22:51 ===============
=> Current Lr: 0.0005
[0/198]: 0.0363
[20/198]: 0.0361
[40/198]: 0.0524
[60/198]: 0.0190
[80/198]: 0.0333
[100/198]: 0.0365
[120/198]: 0.0413
[140/198]: 0.0504
[160/198]: 0.0373
[180/198]: 0.0448
=> Training Loss: 0.0559, Evaluation Loss 0.0388

============= Epoch 8 | 2022-09-02 17:23:58 ===============
=> Current Lr: 0.0005
[0/198]: 0.0302
[20/198]: 0.0572
[40/198]: 0.0853
[60/198]: 0.0638
[80/198]: 0.0775
[100/198]: 0.0638
[120/198]: 0.0586
[140/198]: 0.0404
[160/198]: 0.0232
[180/198]: 0.0513
=> Training Loss: 0.0492, Evaluation Loss 0.0561

============= Epoch 9 | 2022-09-02 17:25:04 ===============
=> Current Lr: 0.0005
[0/198]: 0.0415
[20/198]: 0.0554
[40/198]: 0.0577
[60/198]: 0.2038
[80/198]: 0.0575
[100/198]: 0.0430
[120/198]: 0.0321
[140/198]: 0.0742
[160/198]: 0.0450
[180/198]: 0.0422
=> Training Loss: 0.0459, Evaluation Loss 0.0433

============= Epoch 10 | 2022-09-02 17:26:11 ==============
=> Current Lr: 0.00025
[0/198]: 0.0591
============= Epoch 10 | 2022-09-02 17:45:12 ==============
=> Current Lr: 0.00025
[0/198]: 0.0215
[20/198]: 0.0293
[40/198]: 0.0202
[60/198]: 0.0531
[80/198]: 0.0180
[100/198]: 0.0378
[120/198]: 0.0290
[140/198]: 0.0221
[160/198]: 0.0143
[180/198]: 0.0182
=> Training Loss: 0.0337, Evaluation Loss 0.0262

============= Epoch 11 | 2022-09-02 17:46:48 ==============
=> Current Lr: 0.00025
[0/198]: 0.0401
[20/198]: 0.0205
[40/198]: 0.0122
[60/198]: 0.0277
[80/198]: 0.0194
[100/198]: 0.0332
[120/198]: 0.0168
[140/198]: 0.0349
[160/198]: 0.0334
[180/198]: 0.0212
=> Training Loss: 0.0258, Evaluation Loss 0.0240

============= Epoch 12 | 2022-09-02 17:48:14 ==============
=> Current Lr: 0.00025
[0/198]: 0.0228
[20/198]: 0.0405
[40/198]: 0.0516
[60/198]: 0.0440
[80/198]: 0.0170
[100/198]: 0.0147
[120/198]: 0.0281
[140/198]: 0.0185
[160/198]: 0.0200
[180/198]: 0.0327
=> Training Loss: 0.0269, Evaluation Loss 0.0220

============= Epoch 13 | 2022-09-02 17:49:39 ==============
=> Current Lr: 0.00025
[0/198]: 0.0137
[20/198]: 0.0204
[40/198]: 0.0180
[60/198]: 0.0625
[80/198]: 0.0176
[100/198]: 0.0140
[120/198]: 0.0163
[140/198]: 0.0366
[160/198]: 0.0190
[180/198]: 0.0140
=> Training Loss: 0.0290, Evaluation Loss 0.0272

============= Epoch 14 | 2022-09-02 17:51:05 ==============
=> Current Lr: 0.00025
[0/198]: 0.0141
[20/198]: 0.0242
[40/198]: 0.0356
[60/198]: 0.0209
[80/198]: 0.0165
[100/198]: 0.0211
[120/198]: 0.0278
[140/198]: 0.0246
[160/198]: 0.0154
[180/198]: 0.0327
=> Training Loss: 0.0285, Evaluation Loss 0.0253

============= Epoch 15 | 2022-09-02 17:52:30 ==============
=> Current Lr: 0.000125
[0/198]: 0.0111
[20/198]: 0.0398
[40/198]: 0.0130
[60/198]: 0.0125
[80/198]: 0.0168
[100/198]: 0.0145
[120/198]: 0.0152
[140/198]: 0.0173
[160/198]: 0.0144
[180/198]: 0.0409
=> Training Loss: 0.0184, Evaluation Loss 0.0160

============= Epoch 16 | 2022-09-02 17:53:56 ==============
=> Current Lr: 0.000125
[0/198]: 0.0310
[20/198]: 0.0174
[40/198]: 0.0110
[60/198]: 0.0612
[80/198]: 0.0340
[100/198]: 0.0118
[120/198]: 0.0189
[140/198]: 0.0093
[160/198]: 0.0212
[180/198]: 0.0199
=> Training Loss: 0.0162, Evaluation Loss 0.0160

============= Epoch 17 | 2022-09-02 17:55:22 ==============
=> Current Lr: 0.000125
[0/198]: 0.0109
[20/198]: 0.0150
[40/198]: 0.0219
[60/198]: 0.0158
[80/198]: 0.0107
[100/198]: 0.0098
[120/198]: 0.0319
[140/198]: 0.0323
[160/198]: 0.0098
[180/198]: 0.0123
=> Training Loss: 0.0159, Evaluation Loss 0.0137

============= Epoch 18 | 2022-09-02 17:56:48 ==============
=> Current Lr: 0.000125
[0/198]: 0.0117
[20/198]: 0.0124
[40/198]: 0.0115
[60/198]: 0.0117
[80/198]: 0.0106
[100/198]: 0.0191
[120/198]: 0.0104
[140/198]: 0.0132
[160/198]: 0.0107
[180/198]: 0.0522
=> Training Loss: 0.0144, Evaluation Loss 0.0139

============= Epoch 19 | 2022-09-02 17:58:14 ==============
=> Current Lr: 0.000125
[0/198]: 0.0172
[20/198]: 0.0104
[40/198]: 0.0121
[60/198]: 0.0109
[80/198]: 0.0103
[100/198]: 0.0248
[120/198]: 0.0111
[140/198]: 0.0111
[160/198]: 0.0122
[180/198]: 0.0134
=> Training Loss: 0.0147, Evaluation Loss 0.0146

============= Epoch 20 | 2022-09-02 17:59:39 ==============
=> Current Lr: 6.25e-05
[0/198]: 0.0433
[20/198]: 0.0118
[40/198]: 0.0162
[60/198]: 0.0095
[80/198]: 0.0097
[100/198]: 0.0111
[120/198]: 0.0086
[140/198]: 0.0131
[160/198]: 0.0114
[180/198]: 0.0097
=> Training Loss: 0.0133, Evaluation Loss 0.0110

============= Epoch 21 | 2022-09-02 18:01:06 ==============
=> Current Lr: 6.25e-05
[0/198]: 0.0106
[20/198]: 0.0106
[40/198]: 0.0121
[60/198]: 0.0109
[80/198]: 0.0185
[100/198]: 0.0101
[120/198]: 0.0109
[140/198]: 0.0102
[160/198]: 0.0144
[180/198]: 0.0129
=> Training Loss: 0.0114, Evaluation Loss 0.0112

============= Epoch 22 | 2022-09-02 18:02:29 ==============
=> Current Lr: 6.25e-05
[0/198]: 0.0107
[20/198]: 0.0101
[40/198]: 0.0101
[60/198]: 0.0267
[80/198]: 0.0095
[100/198]: 0.0091
[120/198]: 0.0147
[140/198]: 0.0154
[160/198]: 0.0331
[180/198]: 0.0108
=> Training Loss: 0.0113, Evaluation Loss 0.0106

============= Epoch 23 | 2022-09-02 18:03:57 ==============
=> Current Lr: 6.25e-05
[0/198]: 0.0092
[20/198]: 0.0112
[40/198]: 0.0108
[60/198]: 0.0229
[80/198]: 0.0280
[100/198]: 0.0138
[120/198]: 0.0108
[140/198]: 0.0095
[160/198]: 0.0082
[180/198]: 0.0124
=> Training Loss: 0.0107, Evaluation Loss 0.0097

============= Epoch 24 | 2022-09-02 18:05:24 ==============
=> Current Lr: 6.25e-05
[0/198]: 0.0088
[20/198]: 0.0100
[40/198]: 0.0092
[60/198]: 0.0088
[80/198]: 0.0100
[100/198]: 0.0091
[120/198]: 0.0091
[140/198]: 0.0102
[160/198]: 0.0089
[180/198]: 0.0108
=> Training Loss: 0.0106, Evaluation Loss 0.0110

============= Epoch 25 | 2022-09-02 18:06:51 ==============
=> Current Lr: 3.125e-05
[0/198]: 0.0098
[20/198]: 0.0125
[40/198]: 0.0112
[60/198]: 0.0125
[80/198]: 0.0099
[100/198]: 0.0096
[120/198]: 0.0091
[140/198]: 0.0104
[160/198]: 0.0118
[180/198]: 0.0110
=> Training Loss: 0.0107, Evaluation Loss 0.0106

============= Epoch 26 | 2022-09-02 18:08:16 ==============
=> Current Lr: 3.125e-05
[0/198]: 0.0115
[20/198]: 0.0103
[40/198]: 0.0092
[60/198]: 0.0099
[80/198]: 0.0087
[100/198]: 0.0078
[120/198]: 0.0084
[140/198]: 0.0091
[160/198]: 0.0088
[180/198]: 0.0094
=> Training Loss: 0.0100, Evaluation Loss 0.0095

============= Epoch 27 | 2022-09-02 18:09:43 ==============
=> Current Lr: 3.125e-05
[0/198]: 0.0108
[20/198]: 0.0084
[40/198]: 0.0126
[60/198]: 0.0099
[80/198]: 0.0092
[100/198]: 0.0114
[120/198]: 0.0089
[140/198]: 0.0231
[160/198]: 0.0094
[180/198]: 0.0096
=> Training Loss: 0.0103, Evaluation Loss 0.0093

============= Epoch 28 | 2022-09-02 18:11:08 ==============
=> Current Lr: 3.125e-05
[0/198]: 0.0117
[20/198]: 0.0104
[40/198]: 0.0090
[60/198]: 0.0088
[80/198]: 0.0107
[100/198]: 0.0092
[120/198]: 0.0094
[140/198]: 0.0095
[160/198]: 0.0102
[180/198]: 0.0092
=> Training Loss: 0.0100, Evaluation Loss 0.0094

============= Epoch 29 | 2022-09-02 18:12:32 ==============
=> Current Lr: 3.125e-05
[0/198]: 0.0081
[20/198]: 0.0099
============= Epoch 29 | 2022-09-02 21:24:37 ==============
=> Current Lr: 3.125e-05
[0/1188]: 0.5942
[20/1188]: 0.1280
[40/1188]: 0.0738
[60/1188]: 0.0588
[80/1188]: 0.0621
[100/1188]: 0.0591
[120/1188]: 0.0489
[140/1188]: 0.0506
[160/1188]: 0.0417
[180/1188]: 0.0481
[200/1188]: 0.0358
[220/1188]: 0.0448
[240/1188]: 0.0378
[260/1188]: 0.0292
[280/1188]: 0.0403
[300/1188]: 0.0331
[320/1188]: 0.0341
[340/1188]: 0.0340
[360/1188]: 0.0332
[380/1188]: 0.0415
[400/1188]: 0.0329
[420/1188]: 0.0371
[440/1188]: 0.0390
[460/1188]: 0.0342
[480/1188]: 0.0316
[500/1188]: 0.0249
[520/1188]: 0.0269
[540/1188]: 0.0347
[560/1188]: 0.0391
[580/1188]: 0.0373
[600/1188]: 0.0312
[620/1188]: 0.0332
[640/1188]: 0.0245
[660/1188]: 0.0307
[680/1188]: 0.0249
[700/1188]: 0.0291
[720/1188]: 0.0258
[740/1188]: 0.0283
[760/1188]: 0.0409
[780/1188]: 0.0358
[800/1188]: 0.0236
[820/1188]: 0.0386
[840/1188]: 0.0195
[860/1188]: 0.0208
[880/1188]: 0.0244
[900/1188]: 0.0716
[920/1188]: 0.0242
[940/1188]: 0.0279
[960/1188]: 0.0235
[980/1188]: 0.0362
[1000/1188]: 0.0367
[1020/1188]: 0.0188
[1040/1188]: 0.0219
[1060/1188]: 0.0164
[1080/1188]: 0.0234
[1100/1188]: 0.0239
[1120/1188]: 0.0287
[1140/1188]: 0.0244
[1160/1188]: 0.0223
[1180/1188]: 0.0287
=> Training Loss: 0.0398, Evaluation Loss 0.0229

============= Epoch 30 | 2022-09-02 21:32:59 ==============
=> Current Lr: 1.5625e-05
[0/1188]: 0.0204
[20/1188]: 0.0322
[40/1188]: 0.0201
[60/1188]: 0.0135
[80/1188]: 0.0208
[100/1188]: 0.0298
[120/1188]: 0.0159
[140/1188]: 0.0219
[160/1188]: 0.0193
[180/1188]: 0.0269
[200/1188]: 0.0270
[220/1188]: 0.0351
[240/1188]: 0.0203
[260/1188]: 0.0434
[280/1188]: 0.0206
[300/1188]: 0.0314
[320/1188]: 0.0122
[340/1188]: 0.0210
[360/1188]: 0.0194
[380/1188]: 0.0173
[400/1188]: 0.0180
[420/1188]: 0.0340
[440/1188]: 0.0203
[460/1188]: 0.0340
[480/1188]: 0.0212
[500/1188]: 0.0201
[520/1188]: 0.0168
[540/1188]: 0.0147
[560/1188]: 0.0241
[580/1188]: 0.0276
[600/1188]: 0.0254
[620/1188]: 0.0132
[640/1188]: 0.0225
[660/1188]: 0.0131
[680/1188]: 0.0160
[700/1188]: 0.0154
[720/1188]: 0.0194
[740/1188]: 0.0257
[760/1188]: 0.0278
[780/1188]: 0.0181
[800/1188]: 0.0161
[820/1188]: 0.0196
[840/1188]: 0.0203
[860/1188]: 0.0158
[880/1188]: 0.0282
[900/1188]: 0.0168
[920/1188]: 0.0172
[940/1188]: 0.0158
[960/1188]: 0.0147
[980/1188]: 0.0194
[1000/1188]: 0.0145
[1020/1188]: 0.0230
[1040/1188]: 0.0296
[1060/1188]: 0.0146
[1080/1188]: 0.0131
[1100/1188]: 0.0216
[1120/1188]: 0.0208
[1140/1188]: 0.0247
[1160/1188]: 0.0146
[1180/1188]: 0.0202
=> Training Loss: 0.0206, Evaluation Loss 0.0183

============= Epoch 31 | 2022-09-02 21:41:15 ==============
=> Current Lr: 1.5625e-05
[0/1188]: 0.0196
[20/1188]: 0.0148
[40/1188]: 0.0252
[60/1188]: 0.0115
[80/1188]: 0.0206
[100/1188]: 0.0141
[120/1188]: 0.0176
[140/1188]: 0.0200
[160/1188]: 0.0145
[180/1188]: 0.0185
[200/1188]: 0.0129
[220/1188]: 0.0234
[240/1188]: 0.0208
[260/1188]: 0.0098
[280/1188]: 0.0244
[300/1188]: 0.0141
[320/1188]: 0.0186
[340/1188]: 0.0225
[360/1188]: 0.0122
[380/1188]: 0.0094
[400/1188]: 0.0198
[420/1188]: 0.0169
[440/1188]: 0.0152
[460/1188]: 0.0138
[480/1188]: 0.0148
[500/1188]: 0.0210
[520/1188]: 0.0138
[540/1188]: 0.0178
[560/1188]: 0.0123
[580/1188]: 0.0117
[600/1188]: 0.0127
[620/1188]: 0.0144
[640/1188]: 0.0299
[660/1188]: 0.0168
[680/1188]: 0.0196
[700/1188]: 0.0165
[720/1188]: 0.0227
[740/1188]: 0.0180
[760/1188]: 0.0262
[780/1188]: 0.0142
[800/1188]: 0.0141
[820/1188]: 0.0159
[840/1188]: 0.0120
[860/1188]: 0.0107
[880/1188]: 0.0120
[900/1188]: 0.0225
[920/1188]: 0.0207
[940/1188]: 0.0115
[960/1188]: 0.0290
[980/1188]: 0.0164
[1000/1188]: 0.0147
[1020/1188]: 0.0193
[1040/1188]: 0.0211
[1060/1188]: 0.0112
[1080/1188]: 0.0132
[1100/1188]: 0.0116
[1120/1188]: 0.0086
[1140/1188]: 0.0199
[1160/1188]: 0.0286
[1180/1188]: 0.0134
=> Training Loss: 0.0177, Evaluation Loss 0.0169

============= Epoch 32 | 2022-09-02 21:49:22 ==============
=> Current Lr: 1.5625e-05
[0/1188]: 0.0240
[20/1188]: 0.0244
[40/1188]: 0.0207
[60/1188]: 0.0163
[80/1188]: 0.0152
[100/1188]: 0.0322
[120/1188]: 0.0166
[140/1188]: 0.0157
[160/1188]: 0.0247
[180/1188]: 0.0132
[200/1188]: 0.0158
[220/1188]: 0.0157
[240/1188]: 0.0145
[260/1188]: 0.0138
[280/1188]: 0.0116
[300/1188]: 0.0197
[320/1188]: 0.0123
[340/1188]: 0.0184
[360/1188]: 0.0090
[380/1188]: 0.0118
[400/1188]: 0.0201
[420/1188]: 0.0152
[440/1188]: 0.0119
[460/1188]: 0.0119
[480/1188]: 0.0122
[500/1188]: 0.0173
[520/1188]: 0.0162
[540/1188]: 0.0108
[560/1188]: 0.0160
[580/1188]: 0.0154
[600/1188]: 0.0125
[620/1188]: 0.0171
[640/1188]: 0.0122
[660/1188]: 0.0135
[680/1188]: 0.0102
[700/1188]: 0.0124
[720/1188]: 0.0186
[740/1188]: 0.0171
[760/1188]: 0.0134
[780/1188]: 0.0269
[800/1188]: 0.0129
[820/1188]: 0.0107
[840/1188]: 0.0134
[860/1188]: 0.0176
[880/1188]: 0.0087
[900/1188]: 0.0247
[920/1188]: 0.0108
[940/1188]: 0.0123
[960/1188]: 0.0207
[980/1188]: 0.0138
[1000/1188]: 0.0117
[1020/1188]: 0.0107
[1040/1188]: 0.0164
[1060/1188]: 0.0123
[1080/1188]: 0.0242
[1100/1188]: 0.0129
[1120/1188]: 0.0106
[1140/1188]: 0.0177
[1160/1188]: 0.0159
[1180/1188]: 0.0117
=> Training Loss: 0.0162, Evaluation Loss 0.0147

============= Epoch 33 | 2022-09-02 21:57:34 ==============
=> Current Lr: 1.5625e-05
[0/1188]: 0.0096
[20/1188]: 0.0130
[40/1188]: 0.0180
[60/1188]: 0.0197
[80/1188]: 0.0144
[100/1188]: 0.0170
[120/1188]: 0.0169
[140/1188]: 0.0129
[160/1188]: 0.0137
[180/1188]: 0.0086
[200/1188]: 0.0103
[220/1188]: 0.0109
[240/1188]: 0.0138
[260/1188]: 0.0227
[280/1188]: 0.0197
[300/1188]: 0.0121
[320/1188]: 0.0208
[340/1188]: 0.0113
[360/1188]: 0.0184
[380/1188]: 0.0124
[400/1188]: 0.0149
[420/1188]: 0.0145
[440/1188]: 0.0149
[460/1188]: 0.0146
[480/1188]: 0.0133
[500/1188]: 0.0100
[520/1188]: 0.0105
[540/1188]: 0.0141
[560/1188]: 0.0100
[580/1188]: 0.0080
[600/1188]: 0.0119
[620/1188]: 0.0232
[640/1188]: 0.0149
[660/1188]: 0.0108
[680/1188]: 0.0110
[700/1188]: 0.0101
[720/1188]: 0.0158
[740/1188]: 0.0123
[760/1188]: 0.0138
[780/1188]: 0.0167
[800/1188]: 0.0113
[820/1188]: 0.0214
[840/1188]: 0.0156
[860/1188]: 0.0150
[880/1188]: 0.0166
[900/1188]: 0.0163
[920/1188]: 0.0109
[940/1188]: 0.0126
[960/1188]: 0.0104
[980/1188]: 0.0154
[1000/1188]: 0.0218
[1020/1188]: 0.0097
[1040/1188]: 0.0110
[1060/1188]: 0.0137
[1080/1188]: 0.0158
[1100/1188]: 0.0198
[1120/1188]: 0.0088
[1140/1188]: 0.0112
[1160/1188]: 0.0120
[1180/1188]: 0.0150
=> Training Loss: 0.0150, Evaluation Loss 0.0141

============= Epoch 34 | 2022-09-02 22:05:43 ==============
=> Current Lr: 1.5625e-05
[0/1188]: 0.0113
[20/1188]: 0.0133
[40/1188]: 0.0164
[60/1188]: 0.0112
[80/1188]: 0.0120
[100/1188]: 0.0146
[120/1188]: 0.0126
[140/1188]: 0.0096
[160/1188]: 0.0154
[180/1188]: 0.0147
[200/1188]: 0.0189
[220/1188]: 0.0136
[240/1188]: 0.0119
[260/1188]: 0.0097
[280/1188]: 0.0096
[300/1188]: 0.0116
[320/1188]: 0.0117
[340/1188]: 0.0107
[360/1188]: 0.0147
[380/1188]: 0.0124
[400/1188]: 0.0086
[420/1188]: 0.0207
[440/1188]: 0.0148
[460/1188]: 0.0108
[480/1188]: 0.0086
[500/1188]: 0.0075
[520/1188]: 0.0114
[540/1188]: 0.0146
[560/1188]: 0.0084
[580/1188]: 0.0119
[600/1188]: 0.0092
[620/1188]: 0.0132
[640/1188]: 0.0156
[660/1188]: 0.0113
[680/1188]: 0.0081
[700/1188]: 0.0126
[720/1188]: 0.0129
[740/1188]: 0.0107
[760/1188]: 0.0182
[780/1188]: 0.0114
[800/1188]: 0.0175
[820/1188]: 0.0135
[840/1188]: 0.0099
[860/1188]: 0.0107
[880/1188]: 0.0130
[900/1188]: 0.0074
[920/1188]: 0.0108
[940/1188]: 0.0110
[960/1188]: 0.0175
[980/1188]: 0.0104
[1000/1188]: 0.0145
[1020/1188]: 0.0236
[1040/1188]: 0.0115
[1060/1188]: 0.0093
[1080/1188]: 0.0186
[1100/1188]: 0.0201
[1120/1188]: 0.0112
[1140/1188]: 0.0131
[1160/1188]: 0.0121
[1180/1188]: 0.0132
=> Training Loss: 0.0139, Evaluation Loss 0.0141

============= Epoch 35 | 2022-09-02 22:13:57 ==============
=> Current Lr: 7.8125e-06
[0/1188]: 0.0180
[20/1188]: 0.0232
[40/1188]: 0.0124
[60/1188]: 0.0162
[80/1188]: 0.0149
[100/1188]: 0.0096
[120/1188]: 0.0078
[140/1188]: 0.0126
[160/1188]: 0.0195
[180/1188]: 0.0094
[200/1188]: 0.0089
[220/1188]: 0.0131
[240/1188]: 0.0117
[260/1188]: 0.0090
[280/1188]: 0.0100
[300/1188]: 0.0169
[320/1188]: 0.0103
[340/1188]: 0.0113
[360/1188]: 0.0153
[380/1188]: 0.0097
[400/1188]: 0.0085
[420/1188]: 0.0194
[440/1188]: 0.0108
[460/1188]: 0.0192
[480/1188]: 0.0113
[500/1188]: 0.0161
[520/1188]: 0.0066
[540/1188]: 0.0101
[560/1188]: 0.0099
[580/1188]: 0.0126
[600/1188]: 0.0155
[620/1188]: 0.0097
[640/1188]: 0.0141
[660/1188]: 0.0120
[680/1188]: 0.0085
[700/1188]: 0.0188
[720/1188]: 0.0154
[740/1188]: 0.0136
[760/1188]: 0.0093
[780/1188]: 0.0117
[800/1188]: 0.0069
[820/1188]: 0.0073
[840/1188]: 0.0193
[860/1188]: 0.0115
[880/1188]: 0.0094
[900/1188]: 0.0124
[920/1188]: 0.0216
[940/1188]: 0.0153
[960/1188]: 0.0140
[980/1188]: 0.0178
[1000/1188]: 0.0167
[1020/1188]: 0.0119
[1040/1188]: 0.0188
[1060/1188]: 0.0144
[1080/1188]: 0.0204
[1100/1188]: 0.0118
[1120/1188]: 0.0117
[1140/1188]: 0.0105
[1160/1188]: 0.0125
[1180/1188]: 0.0112
=> Training Loss: 0.0126, Evaluation Loss 0.0123

============= Epoch 36 | 2022-09-02 22:22:01 ==============
=> Current Lr: 7.8125e-06
[0/1188]: 0.0136
[20/1188]: 0.0093
[40/1188]: 0.0140
[60/1188]: 0.0115
[80/1188]: 0.0148
[100/1188]: 0.0139
[120/1188]: 0.0130
[140/1188]: 0.0101
[160/1188]: 0.0141
[180/1188]: 0.0110
[200/1188]: 0.0139
[220/1188]: 0.0157
[240/1188]: 0.0151
[260/1188]: 0.0160
[280/1188]: 0.0104
[300/1188]: 0.0124
[320/1188]: 0.0089
[340/1188]: 0.0074
[360/1188]: 0.0141
[380/1188]: 0.0113
[400/1188]: 0.0066
[420/1188]: 0.0198
[440/1188]: 0.0116
[460/1188]: 0.0136
[480/1188]: 0.0071
[500/1188]: 0.0069
[520/1188]: 0.0064
[540/1188]: 0.0109
[560/1188]: 0.0120
[580/1188]: 0.0155
[600/1188]: 0.0124
[620/1188]: 0.0112
[640/1188]: 0.0127
[660/1188]: 0.0114
[680/1188]: 0.0130
[700/1188]: 0.0185
[720/1188]: 0.0080
[740/1188]: 0.0208
[760/1188]: 0.0122
[780/1188]: 0.0099
[800/1188]: 0.0085
[820/1188]: 0.0194
[840/1188]: 0.0124
[860/1188]: 0.0089
[880/1188]: 0.0070
[900/1188]: 0.0118
[920/1188]: 0.0086
[940/1188]: 0.0127
[960/1188]: 0.0118
[980/1188]: 0.0152
[1000/1188]: 0.0164
[1020/1188]: 0.0105
[1040/1188]: 0.0086
[1060/1188]: 0.0080
[1080/1188]: 0.0100
[1100/1188]: 0.0106
[1120/1188]: 0.0093
[1140/1188]: 0.0617
[1160/1188]: 0.0145
[1180/1188]: 0.0058
=> Training Loss: 0.0118, Evaluation Loss 0.0117

============= Epoch 37 | 2022-09-02 22:29:12 ==============
=> Current Lr: 7.8125e-06
[0/1188]: 0.0121
[20/1188]: 0.0081
[40/1188]: 0.0105
[60/1188]: 0.0186
[80/1188]: 0.0114
[100/1188]: 0.0167
[120/1188]: 0.0082
[140/1188]: 0.0113
[160/1188]: 0.0146
[180/1188]: 0.0098
[200/1188]: 0.0098
[220/1188]: 0.0152
[240/1188]: 0.0130
[260/1188]: 0.0131
[280/1188]: 0.0085
[300/1188]: 0.0192
[320/1188]: 0.0133
[340/1188]: 0.0154
[360/1188]: 0.0118
[380/1188]: 0.0082
[400/1188]: 0.0074
[420/1188]: 0.0109
[440/1188]: 0.0110
[460/1188]: 0.0167
[480/1188]: 0.0094
[500/1188]: 0.0078
[520/1188]: 0.0097
[540/1188]: 0.0089
[560/1188]: 0.0087
[580/1188]: 0.0133
[600/1188]: 0.0201
[620/1188]: 0.0085
[640/1188]: 0.0138
[660/1188]: 0.0130
[680/1188]: 0.0122
[700/1188]: 0.0129
[720/1188]: 0.0099
[740/1188]: 0.0182
[760/1188]: 0.0102
[780/1188]: 0.0112
[800/1188]: 0.0218
[820/1188]: 0.0118
[840/1188]: 0.0116
[860/1188]: 0.0132
[880/1188]: 0.0115
[900/1188]: 0.0121
[920/1188]: 0.0143
[940/1188]: 0.0057
[960/1188]: 0.0073
[980/1188]: 0.0067
[1000/1188]: 0.0161
[1020/1188]: 0.0211
[1040/1188]: 0.0075
[1060/1188]: 0.0113
[1080/1188]: 0.0089
[1100/1188]: 0.0104
[1120/1188]: 0.0057
[1140/1188]: 0.0105
[1160/1188]: 0.0080
[1180/1188]: 0.0113
=> Training Loss: 0.0117, Evaluation Loss 0.0115

============= Epoch 38 | 2022-09-02 22:35:03 ==============
=> Current Lr: 7.8125e-06
[0/1188]: 0.0121
[20/1188]: 0.0060
[40/1188]: 0.0076
[60/1188]: 0.0110
[80/1188]: 0.0110
[100/1188]: 0.0120
[120/1188]: 0.0114
[140/1188]: 0.0094
[160/1188]: 0.0179
[180/1188]: 0.0096
[200/1188]: 0.0097
[220/1188]: 0.0200
[240/1188]: 0.0112
[260/1188]: 0.0077
[280/1188]: 0.0132
[300/1188]: 0.0058
[320/1188]: 0.0068
[340/1188]: 0.0084
[360/1188]: 0.0157
[380/1188]: 0.0153
[400/1188]: 0.0081
[420/1188]: 0.0129
[440/1188]: 0.0079
[460/1188]: 0.0090
[480/1188]: 0.0159
[500/1188]: 0.0170
[520/1188]: 0.0147
[540/1188]: 0.0103
[560/1188]: 0.0114
[580/1188]: 0.0099
[600/1188]: 0.0116
[620/1188]: 0.0119
[640/1188]: 0.0107
[660/1188]: 0.0119
[680/1188]: 0.0076
[700/1188]: 0.0087
[720/1188]: 0.0179
[740/1188]: 0.0082
[760/1188]: 0.0122
[780/1188]: 0.0105
[800/1188]: 0.0137
[820/1188]: 0.0066
[840/1188]: 0.0100
[860/1188]: 0.0120
[880/1188]: 0.0105
[900/1188]: 0.0113
[920/1188]: 0.0093
[940/1188]: 0.0099
[960/1188]: 0.0116
[980/1188]: 0.0096
[1000/1188]: 0.0113
[1020/1188]: 0.0096
[1040/1188]: 0.0116
[1060/1188]: 0.0112
[1080/1188]: 0.0077
[1100/1188]: 0.0128
[1120/1188]: 0.0095
[1140/1188]: 0.0099
[1160/1188]: 0.0091
[1180/1188]: 0.0167
=> Training Loss: 0.0116, Evaluation Loss 0.0112

============= Epoch 39 | 2022-09-02 22:40:53 ==============
=> Current Lr: 7.8125e-06
[0/1188]: 0.0094
[20/1188]: 0.0107
[40/1188]: 0.0080
[60/1188]: 0.0084
[80/1188]: 0.0066
[100/1188]: 0.0117
[120/1188]: 0.0129
[140/1188]: 0.0117
[160/1188]: 0.0080
[180/1188]: 0.0098
[200/1188]: 0.0077
[220/1188]: 0.0094
[240/1188]: 0.0098
[260/1188]: 0.0078
[280/1188]: 0.0073
[300/1188]: 0.0123
[320/1188]: 0.0087
[340/1188]: 0.0146
[360/1188]: 0.0130
[380/1188]: 0.0064
[400/1188]: 0.0136
[420/1188]: 0.0078
[440/1188]: 0.0188
[460/1188]: 0.0114
[480/1188]: 0.0087
[500/1188]: 0.0109
[520/1188]: 0.0069
[540/1188]: 0.0077
[560/1188]: 0.0090
[580/1188]: 0.0131
[600/1188]: 0.0183
[620/1188]: 0.0106
[640/1188]: 0.0085
[660/1188]: 0.0199
[680/1188]: 0.0146
[700/1188]: 0.0113
[720/1188]: 0.0099
[740/1188]: 0.0111
[760/1188]: 0.0110
[780/1188]: 0.0116
[800/1188]: 0.0065
[820/1188]: 0.0129
[840/1188]: 0.0130
[860/1188]: 0.0132
[880/1188]: 0.0084
[900/1188]: 0.0122
[920/1188]: 0.0113
[940/1188]: 0.0088
[960/1188]: 0.0095
[980/1188]: 0.0125
[1000/1188]: 0.0114
[1020/1188]: 0.0106
[1040/1188]: 0.0116
[1060/1188]: 0.0169
[1080/1188]: 0.0110
[1100/1188]: 0.0085
[1120/1188]: 0.0119
[1140/1188]: 0.0153
[1160/1188]: 0.0096
[1180/1188]: 0.0092
=> Training Loss: 0.0110, Evaluation Loss 0.0109
